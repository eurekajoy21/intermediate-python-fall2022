{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping in Python\n",
    "\n",
    "Web scraping is the process of collecting and parsing raw data from the Web, and the Python community has come up with some pretty powerful web scraping tools.\n",
    "Many disciplines, such as data science, business intelligence, and investigative reporting, can benefit enormously from collecting and analyzing data from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Parse texts from Websites\n",
    "Collecting data from websites using an automated process is known as web scraping. Some websites explicitly forbid users from scraping their data with automated tools.\n",
    "\n",
    "**Websites have two main reasons to not allow web scraping**\n",
    "1. To protect its data. For example: Google maps do not allow users to request too many results in a minute.\n",
    "2. To prevent overuse of their servers. When bots start sending many requests website's servers slow down and thus other users will have slower connection to the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful package for web scraping that you can find in Python’s standard library is [urllib](https://docs.python.org/3/library/urllib.html), which contains tools for working with URLs.\n",
    "**urllib** is for opening and reading URLs.\n",
    "\n",
    "#### Let's look at the example and use **urllib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "page = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the HTML from the page:\n",
    "1. Use html's read method to return sequence of bytes\n",
    "2. Use decode method on 1st result to decode bytes to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html_by = page.read()\n",
    "html = html_by.decode(\"utf-8\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to get the title of the webpage\n",
    "1. We need to get the index of the **\\<title>**, and because title tags strings have been counted we need to add it to the index. \n",
    "2. Find the index of the closing **\\<title>** tag\n",
    "3. Get the title by slicing the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>Profile: Aphrodite</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/aphrodite.gif\" />\\n<h2>Name: Aphrodite</h2>\\n<br><br>\\nFavorite animal: Dove\\n<br><br>\\nFavorite color: Red\\n<br><br>\\nHometown: Mount Olympus\\n</center>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.find(\"<title>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<title>'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[14:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = html.find(\"<title>\")\n",
    "start_index = title_index + len(\"<title>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(start_index)\n",
    "print(title_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "end_index = html.find(\"</title>\")\n",
    "print(end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "title = html[start_index:end_index]\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is a lot of work just to get the title of the page. In the real world, websites are much more complex and complicated. We can use find many dedicated tools for html scraping but the most powerful and popular library for Python is [**Beautiful soup**](https://www.crummy.com/software/BeautifulSoup/)\n",
    "\n",
    "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping.\n",
    "\n",
    "**Run the command below to install**:\n",
    "```bash\n",
    "conda install beautifulsoup4\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example above does three things\n",
    "1. Opens up a page using **urlopen** from **urllib.request**\n",
    "2. Reads and decodes the page and saves as a variable\n",
    "3. Creates a BeautifulSoup object and assigns it to the soup variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup objects have a **.get_text()** method that can be used to extract all the text from the document and automatically remove any HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Aphrodite\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Aphrodite\n",
      "\n",
      "Favorite animal: Dove\n",
      "\n",
      "Favorite color: Red\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the title of the page, you can use **.title**, and **.string** to get the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Profile: Aphrodite</title>\n",
      "Profile: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **find()** to find the tags you want and get the source attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = soup.find(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img src=\"/static/aphrodite.gif\"/>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/aphrodite.gif'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise your web scraping on Unegui.mn\n",
    "1. Go to https://www.unegui.mn/avto-mashin/-avtomashin-zarna/, Use inspection tool on your browser to see the html tags and attributes.\n",
    "2. Scrape all the listing's **title** and **price**. Scrape only the first page!\n",
    "3. Save your listings as a pandas DataFrame\n",
    "Example below illustrates the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titles = ['Toyota FJ Cruiser, 2012/2020', 'Honda Crossroad, 2009/2019']\n",
    "prices = ['62 сая', '17 сая']\n",
    "results = pd.DataFrame([titles, prices], columns=['titles', 'prices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "announcement-block__price _verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.unegui.mn/kompyuter-busad/notebook/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all(\"div\", {\"class\": \"announcement-block__price _verified\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<meta content=\"2390000.00\" itemprop=\"price\"/>]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].find_all(\"meta\", {\"itemprop\":\"price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.unegui.mn/kompyuter-busad/notebook/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.unegui.mn/kompyuter-busad/notebook/?page={}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
